{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preped\n"
     ]
    }
   ],
   "source": [
    "#Extract Data\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from sklearn import metrics \n",
    "\n",
    "datasetFile = open('./spambase.data', 'r')\n",
    "\n",
    "spam = list()\n",
    "notSpam = list()\n",
    "\n",
    "\n",
    "for line in datasetFile:\n",
    "    splitedLine = line.split(',')\n",
    "    for a in range(0, len(splitedLine)):\n",
    "        splitedLine[a] = float(splitedLine[a])\n",
    "    if(splitedLine[-1] == 1):\n",
    "        spam.append(splitedLine)\n",
    "    else:\n",
    "        notSpam.append(splitedLine)\n",
    "\n",
    "#Randomize\n",
    "random.shuffle(spam)\n",
    "random.shuffle(notSpam)\n",
    "\n",
    "training = copy.deepcopy(spam[0:1359])\n",
    "\n",
    "notSpamTraining = copy.deepcopy(notSpam[0:2091])\n",
    "training.extend(notSpamTraining)\n",
    "\n",
    "testing = copy.deepcopy(spam[1359:-1])\n",
    "\n",
    "notSpamTesting = copy.deepcopy(notSpam[2091:-1])\n",
    "testing.extend(notSpamTesting)\n",
    "\n",
    "random.shuffle(training)\n",
    "random.shuffle(testing)\n",
    "    \n",
    "trainingX = np.array([row[:-1] for row in training])\n",
    "trainingY = np.array([row[-1] for row in training])\n",
    "\n",
    "testingX = np.array([row[:-1] for row in testing])\n",
    "testingY = np.array([row[-1] for row in testing])\n",
    "\n",
    "print(\"Data Preped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy 0.94\n",
      "Error 0.06\n",
      "\n",
      "ROC Curve\n",
      "For Threshold  0.1\n",
      "True Positive Rate: 0.9912\n",
      "False Positive Rate: 0.3261\n",
      "\n",
      "For Threshold  0.2\n",
      "True Positive Rate: 0.9823\n",
      "False Positive Rate: 0.2055\n",
      "\n",
      "For Threshold  0.3\n",
      "True Positive Rate: 0.9735\n",
      "False Positive Rate: 0.1164\n",
      "\n",
      "For Threshold  0.4\n",
      "True Positive Rate: 0.9581\n",
      "False Positive Rate: 0.0747\n",
      "\n",
      "For Threshold  0.5\n",
      "True Positive Rate: 0.9294\n",
      "False Positive Rate: 0.0474\n",
      "\n",
      "For Threshold  0.6\n",
      "True Positive Rate: 0.8808\n",
      "False Positive Rate: 0.0345\n",
      "\n",
      "For Threshold  0.7\n",
      "True Positive Rate: 0.8124\n",
      "False Positive Rate: 0.0259\n",
      "\n",
      "For Threshold  0.8\n",
      "True Positive Rate: 0.7439\n",
      "False Positive Rate: 0.0172\n",
      "\n",
      "For Threshold  0.9\n",
      "True Positive Rate: 0.6225\n",
      "False Positive Rate: 0.0129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics \n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(trainingX, trainingY)\n",
    "\n",
    "y_pred = logreg.predict(testingX)\n",
    "\n",
    "confMatrix = metrics.confusion_matrix(testingY, y_pred)\n",
    "accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "print(\"Logistic Regression\")\n",
    "print(\"Accuracy %.2f\" % accuracy)\n",
    "print(\"Error %.2f\" % (1-accuracy))\n",
    "print()\n",
    "\n",
    "print(\"ROC Curve\")\n",
    "for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    pred_Y = list()\n",
    "    proba = logreg.predict_proba(testingX)\n",
    "    for a in proba:\n",
    "        if(a[1] < threshold):\n",
    "            pred_Y.append(0)\n",
    "        else:\n",
    "            pred_Y.append(1)\n",
    "            \n",
    "    confMatrix = metrics.confusion_matrix(testingY, pred_Y)\n",
    "    tpr = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "    fpr = confMatrix[0][1]/(confMatrix[0][1]+confMatrix[0][0])\n",
    "    \n",
    "    print(\"For Threshold \", threshold)\n",
    "    print(\"True Positive Rate: %.4f\" % tpr)\n",
    "    print(\"False Positive Rate: %.4f\" % fpr )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA\n",
      "Accuracy 0.91\n",
      "Error 0.09\n",
      "\n",
      "ROC Curve\n",
      "For Threshold  0.1\n",
      "True Positive Rate: 0.9757\n",
      "False Positive Rate: 0.2859\n",
      "\n",
      "For Threshold  0.2\n",
      "True Positive Rate: 0.9514\n",
      "False Positive Rate: 0.1264\n",
      "\n",
      "For Threshold  0.3\n",
      "True Positive Rate: 0.9139\n",
      "False Positive Rate: 0.0747\n",
      "\n",
      "For Threshold  0.4\n",
      "True Positive Rate: 0.8874\n",
      "False Positive Rate: 0.0474\n",
      "\n",
      "For Threshold  0.5\n",
      "True Positive Rate: 0.8212\n",
      "False Positive Rate: 0.0374\n",
      "\n",
      "For Threshold  0.6\n",
      "True Positive Rate: 0.7506\n",
      "False Positive Rate: 0.0316\n",
      "\n",
      "For Threshold  0.7\n",
      "True Positive Rate: 0.6887\n",
      "False Positive Rate: 0.0216\n",
      "\n",
      "For Threshold  0.8\n",
      "True Positive Rate: 0.6026\n",
      "False Positive Rate: 0.0158\n",
      "\n",
      "For Threshold  0.9\n",
      "True Positive Rate: 0.5033\n",
      "False Positive Rate: 0.0101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(trainingX, trainingY)\n",
    "    \n",
    "y_pred = lda.predict(testingX)\n",
    "    \n",
    "confMatrix = metrics.confusion_matrix(testingY, y_pred)\n",
    "accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "print(\"LDA\")\n",
    "print(\"Accuracy %.2f\" % accuracy)\n",
    "print(\"Error %.2f\" % (1-accuracy))\n",
    "print() \n",
    "\n",
    "print(\"ROC Curve\")\n",
    "for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    pred_Y = list()\n",
    "    proba = lda.predict_proba(testingX)\n",
    "    for a in proba:\n",
    "        if(a[1] < threshold):\n",
    "            pred_Y.append(0)\n",
    "        else:\n",
    "            pred_Y.append(1)\n",
    "            \n",
    "    confMatrix = metrics.confusion_matrix(testingY, pred_Y)\n",
    "    tpr = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "    fpr = confMatrix[0][1]/(confMatrix[0][1]+confMatrix[0][0])\n",
    "    \n",
    "    print(\"For Threshold \", threshold)\n",
    "    print(\"True Positive Rate: %.4f\" % tpr)\n",
    "    print(\"False Positive Rate: %.4f\" % fpr )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.79286 for KNN=2\n",
      "Accuracy 0.80331 for KNN=3\n",
      "Accuracy 0.78503 for KNN=4\n",
      "Accuracy 0.78329 for KNN=5\n",
      "Accuracy 0.78416 for KNN=6\n",
      "Accuracy 0.77981 for KNN=7\n",
      "Accuracy 0.77285 for KNN=8\n",
      "Accuracy 0.78677 for KNN=9\n",
      "\n",
      "kNN set to 3\n",
      "Accuracy 0.80\n",
      "Error 0.20\n",
      "\n",
      "ROC Curve\n",
      "For Threshold  0.1\n",
      "True Positive Rate: 0.9139\n",
      "False Positive Rate: 0.3549\n",
      "\n",
      "For Threshold  0.2\n",
      "True Positive Rate: 0.9139\n",
      "False Positive Rate: 0.3549\n",
      "\n",
      "For Threshold  0.3\n",
      "True Positive Rate: 0.9139\n",
      "False Positive Rate: 0.3549\n",
      "\n",
      "For Threshold  0.4\n",
      "True Positive Rate: 0.7241\n",
      "False Positive Rate: 0.1451\n",
      "\n",
      "For Threshold  0.5\n",
      "True Positive Rate: 0.7241\n",
      "False Positive Rate: 0.1451\n",
      "\n",
      "For Threshold  0.6\n",
      "True Positive Rate: 0.7241\n",
      "False Positive Rate: 0.1451\n",
      "\n",
      "For Threshold  0.7\n",
      "True Positive Rate: 0.4857\n",
      "False Positive Rate: 0.0489\n",
      "\n",
      "For Threshold  0.8\n",
      "True Positive Rate: 0.4857\n",
      "False Positive Rate: 0.0489\n",
      "\n",
      "For Threshold  0.9\n",
      "True Positive Rate: 0.4857\n",
      "False Positive Rate: 0.0489\n",
      "\n",
      "FPR\n",
      "[0.         0.04885057 1.        ]\n",
      "TPR\n",
      "[0.         0.48565121 1.        ]\n",
      "Threshold\n",
      "[2 1 0]\n",
      "AUC: 0.7184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Knn\n",
    "\n",
    "for a in range(2, 10):\n",
    "    knn = KNeighborsClassifier(n_neighbors = a)\n",
    "    knn.fit(trainingX, trainingY)\n",
    "    \n",
    "    y_pred = knn.predict(testingX)\n",
    "    \n",
    "    confMatrix = metrics.confusion_matrix(testingY, y_pred)  \n",
    "    accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "    print(\"Accuracy %.5f for KNN=%d\" % (accuracy,a))\n",
    "print()\n",
    "    \n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(trainingX, trainingY)\n",
    "    \n",
    "y_pred = knn.predict(testingX)\n",
    "    \n",
    "confMatrix = metrics.confusion_matrix(testingY, y_pred)\n",
    "accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "print(\"kNN set to 3\")\n",
    "print(\"Accuracy %.2f\" % accuracy)\n",
    "print(\"Error %.2f\" % (1-accuracy))\n",
    "print() \n",
    "\n",
    "print(\"ROC Curve\")\n",
    "for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    pred_Y = list()\n",
    "    proba = knn.predict_proba(testingX)\n",
    "    for a in proba:\n",
    "        if(a[1] < threshold):\n",
    "            pred_Y.append(0)\n",
    "        else:\n",
    "            pred_Y.append(1)\n",
    "            \n",
    "    confMatrix = metrics.confusion_matrix(testingY, pred_Y)\n",
    "    tpr = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "    fpr = confMatrix[0][1]/(confMatrix[0][1]+confMatrix[0][0])\n",
    "    \n",
    "    print(\"For Threshold \", threshold)\n",
    "    print(\"True Positive Rate: %.4f\" % tpr)\n",
    "    print(\"False Positive Rate: %.4f\" % fpr )\n",
    "    print()\n",
    "fpr, tpr, threshold= metrics.roc_curve(testingY, pred_Y)\n",
    "print(\"FPR\")\n",
    "print(fpr)\n",
    "print(\"TPR\")\n",
    "print(tpr)\n",
    "print(\"Threshold\")\n",
    "print(threshold)\n",
    "print(\"AUC: %.4f\" % metrics.auc(fpr, tpr))    \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "Accuracy 0.91\n",
      "Error 0.09\n",
      "\n",
      "ROC Curve\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "For Threshold  0.1\n",
      "True Positive Rate: 0.8830\n",
      "False Positive Rate: 0.0733\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "bad input shape (1149, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b0dd39d1576d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"True Positive Rate: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"False Positive Rate: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfpr\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestingY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[0;32m    532\u001b[0m     \"\"\"\n\u001b[0;32m    533\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[1;32m--> 534\u001b[1;33m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[1;31m# Attempt to drop thresholds corresponding to points in between and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[1;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 614\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bad input shape {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: bad input shape (1149, 2)"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(trainingX, trainingY)\n",
    "    \n",
    "y_pred = tree.predict(testingX)\n",
    "    \n",
    "confMatrix = metrics.confusion_matrix(testingY, y_pred)\n",
    "accuracy = (confMatrix[1][1]+confMatrix[0][0])/len(y_pred)\n",
    "print(\"Decision Tree\")\n",
    "print(\"Accuracy %.2f\" % accuracy)\n",
    "print(\"Error %.2f\" % (1-accuracy))\n",
    "print() \n",
    "\n",
    "print(\"ROC Curve\")\n",
    "for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    pred_Y = list()\n",
    "    proba = tree.predict_proba(testingX)\n",
    "    for a in proba:\n",
    "        if(a[1] < threshold):\n",
    "            pred_Y.append(0)\n",
    "        else:\n",
    "            pred_Y.append(1)\n",
    "            \n",
    "    confMatrix = metrics.confusion_matrix(testingY, pred_Y)\n",
    "    tpr = confMatrix[1][1]/(confMatrix[1][1]+confMatrix[1][0])\n",
    "    fpr = confMatrix[0][1]/(confMatrix[0][1]+confMatrix[0][0])\n",
    "    \n",
    "    print(\"For Threshold \", threshold)\n",
    "    print(\"True Positive Rate: %.4f\" % tpr)\n",
    "    print(\"False Positive Rate: %.4f\" % fpr )\n",
    "    fpr, tpr, threshold= metrics.roc_curve(testingY, pred_Y)\n",
    "    print(fpr)\n",
    "    print(tpr)\n",
    "    print\n",
    "    print(\"AUC: %.4f\" % metrics.auc(fpr, tpr))    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
